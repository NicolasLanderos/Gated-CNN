{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fZOMjk-2pGPs"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "feuqKVkHqQVW"
   },
   "outputs": [],
   "source": [
    "# Loading Data\n",
    "(_, _), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "# Adding Channel Lenght Dimension: Expanding from (28x28) to (28x28x1)\n",
    "x_test = tf.expand_dims(x_test, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dataset Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z9YPFldhqV5K"
   },
   "outputs": [],
   "source": [
    "# Normalize images\n",
    "def normalize_img(x_, y_):\n",
    "    return tf.cast(x_, tf.float32) / 255., y_\n",
    "\n",
    "# 1-hot encoding\n",
    "def to_categorical(x_, y_):\n",
    "    return x_, tf.one_hot(y_, depth=10)\n",
    "\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_dataset = test_dataset.map(normalize_img)\n",
    "test_dataset = test_dataset.map(to_categorical)\n",
    "test_dataset = test_dataset.batch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Quantization_layer(tensor, Quantization = True,signed = True, word_size = 12, frac_size = 6):\n",
    "    \n",
    "    factor = 2.0**frac_size\n",
    "    \n",
    "    # Quantized max and min values, in case of the need to implement overflow cases.\n",
    "    #if signed:\n",
    "    #    Max_Qvalue = ((1 << (word_size-1)) - 1)/factor\n",
    "    #    Min_Qvalue = -Max_Qvalue - 1\n",
    "    #else:\n",
    "    #    Max_Qvalue = ((1 << (word_size)) - 1)/factor\n",
    "    #    Min_Qvalue = 0\n",
    "    \n",
    "    if Quantization:\n",
    "        return tf.round(tensor*factor) / factor             #Quantization, assuming no overflow\n",
    "    else:\n",
    "        return tensor                                       #Simple Bypass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Lenet Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota: ademas de cuantizarse las entradas, se agrega una capa de cuantizacion luego de cada capa que realice cambios sobre los datos (Conv,Dense,Activacion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_model(input_layer, Quantization = True, signed = True, word_size = 12, frac_size = 6 ):\n",
    "    Arguments = {'Quantization':Quantization, 'signed':signed, 'word_size':word_size, 'frac_size':frac_size}\n",
    "    QInp      = tf.keras.layers.Lambda(Quantization_layer, name=\"QInp\",  arguments = Arguments )(input_layer)\n",
    "    #Conv Block\n",
    "    Conv1   = tf.keras.layers.Conv2D(6, kernel_size=5, strides=1, input_shape=(28,28,1), padding='same', name= 'Conv1')(QInp)\n",
    "    QConv1  = tf.keras.layers.Lambda(Quantization_layer, name=\"QConv1\",  arguments = Arguments )(Conv1)\n",
    "    Act1    = tf.keras.activations.tanh(QConv1)\n",
    "    QAct1   = tf.keras.layers.Lambda(Quantization_layer, name=\"QAct1\",   arguments = Arguments )(Act1)\n",
    "    AvgPool1= tf.keras.layers.AveragePooling2D(name='AvgPool1')(QAct1)\n",
    "    #Conv Block\n",
    "    Conv2   = tf.keras.layers.Conv2D(16, kernel_size=5, strides=1, padding='valid',name='Conv2')(AvgPool1)\n",
    "    QConv2  = tf.keras.layers.Lambda(Quantization_layer, name=\"QConv2\",  arguments = Arguments )(Conv2)\n",
    "    Act2    = tf.keras.activations.tanh(QConv2)\n",
    "    QAct2   = tf.keras.layers.Lambda(Quantization_layer, name=\"QAct2\",   arguments = Arguments )(Act2)\n",
    "    AvgPool2= tf.keras.layers.AveragePooling2D(name='AvgPool2')(QAct2)\n",
    "    Flatten = tf.keras.layers.Flatten(name='Flatten')(AvgPool2)\n",
    "    #Dense Block\n",
    "    Dense1  = tf.keras.layers.Dense(units=120, name='Dense1')(Flatten)\n",
    "    QDense1 = tf.keras.layers.Lambda(Quantization_layer, name=\"QDense1\", arguments = Arguments )(Dense1)\n",
    "    Act3    = tf.keras.activations.tanh(QDense1)\n",
    "    QAct3   = tf.keras.layers.Lambda(Quantization_layer, name=\"QAct3\",   arguments = Arguments )(Act3)\n",
    "    #Dense Block\n",
    "    Dense2  = tf.keras.layers.Dense(units=84, name='Dense2')(QAct3)\n",
    "    QDense2 = tf.keras.layers.Lambda(Quantization_layer, name=\"QDense2\", arguments = Arguments)(Dense2)\n",
    "    Act4    = tf.keras.activations.tanh(QDense2)\n",
    "    QAct4   = tf.keras.layers.Lambda(Quantization_layer, name=\"QAct4\",   arguments = Arguments)(Act4)\n",
    "    #Output Block\n",
    "    Out     = tf.keras.layers.Dense(units=10,name='Output')(QAct4)\n",
    "    QOut    = tf.keras.layers.Lambda(Quantization_layer, name=\"QOut\",    arguments = Arguments)(Out)\n",
    "    Act5    = tf.keras.activations.softmax(QOut)\n",
    "    QAct5   = tf.keras.layers.Lambda(Quantization_layer, name=\"QSoftmax\",arguments = Arguments)(Act5)\n",
    "    \n",
    "    return QAct5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Quantized model and Non Quantized model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_layer   = tf.keras.Input((28, 28, 1))\n",
    "output_layer  = build_model(input_layer, Quantization = False)\n",
    "\n",
    "#For this example we using 3 bits of precision.\n",
    "Qinput_layer  = tf.keras.Input((28, 28, 1))\n",
    "Qoutput_layer = build_model(Qinput_layer, Quantization = True, word_size = 12, frac_size = 3)\n",
    "\n",
    "Lenet  = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "QLenet = tf.keras.Model(inputs=Qinput_layer, outputs=Qoutput_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Pretrained Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Q8qN9xeqzm7"
   },
   "outputs": [],
   "source": [
    "# Loading Wieghts\n",
    "cwd = os.getcwd()\n",
    "Wgt_dir = os.path.join(cwd,'TrainedWeights')\n",
    "Wgt_dir = os.path.join(Wgt_dir,'Weights')\n",
    "\n",
    "Lenet.load_weights(Wgt_dir)\n",
    "QLenet.load_weights(Wgt_dir)\n",
    "# Visualize Lenet 5 Architecture\n",
    "#lenet.summary()\n",
    "\n",
    "# Visualize initialized weights\n",
    "#lenet.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantize Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Quantization(List, Quantization = True, signed = True, word_size = 12, frac_size = 6):\n",
    "    factor = 2.0**frac_size\n",
    "    return tf.round(np.array(List)*factor) / factor             #Quantization, assuming no overflow\n",
    "\n",
    "for layer in QLenet.layers:\n",
    "    weights = layer.get_weights()\n",
    "    if weights:                     # Layer with weights\n",
    "        # Quantization of Weights and Bias \n",
    "        Qweights    = [None,None]\n",
    "        Qweights[0] = Quantization(weights[0], word_size = 12, frac_size = 3)\n",
    "        Qweights[1] = Quantization(weights[1], word_size = 12, frac_size = 3)\n",
    "        layer.set_weights(Qweights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterator over test Dataset\n",
    "iterator  = iter(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting new image from iterator\n",
    "image     = next(iterator)\n",
    "image_plt = image[0][0,...,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Test image\n",
    "plt.imshow(image_plt, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 9\n",
      "Prediction: 9\n",
      "QPrediction: 9\n"
     ]
    }
   ],
   "source": [
    "# Target\n",
    "tf.print(\"Target:\",np.argmax(image[1]))\n",
    "# Predicted Output\n",
    "print(\"Prediction:\",np.argmax(Lenet.predict(image[0])))\n",
    "# Quantized Predicted Output\n",
    "print(\"QPrediction:\",np.argmax(QLenet.predict(image[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparation of Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.4945015e-08, 7.8437586e-07, 4.5743948e-07, 2.4961669e-06,\n",
       "        1.6386039e-06, 1.5448232e-03, 8.7722611e-07, 5.2541746e-03,\n",
       "        1.1379937e-05, 9.9318331e-01]], dtype=float32)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lenet.predict(image[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.125, 0.   ,\n",
       "        0.75 ]], dtype=float32)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QLenet.predict(image[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classes\n",
    "\n",
    "0. T-shirt/top\n",
    "1. Trouser\n",
    "2. Pullover\n",
    "3. Dress\n",
    "4. Coat\n",
    "5. Sandal\n",
    "6. Shirt\n",
    "7. Sneaker\n",
    "8. Bag\n",
    "9. Ankle boot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the general Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization params\n",
    "# -------------------\n",
    "\n",
    "# Loss\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "# learning rate\n",
    "lr = 1e-3\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "# -------------------\n",
    "\n",
    "# Validation metrics\n",
    "# ------------------\n",
    "\n",
    "metrics = ['accuracy']\n",
    "# ------------------\n",
    "\n",
    "# Compile Model\n",
    "Lenet.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "QLenet.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 33s 3ms/step - loss: 0.3196 - accuracy: 0.8883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3196016080860579, 0.8883]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lenet.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 32s 3ms/step - loss: 0.6949 - accuracy: 0.8729\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6948502577155831, 0.8729]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QLenet.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how even with low precicion of the fractional (3 bits) the network accuracy is almost unaffected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking The Output of Each Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.backend import eager_learning_phase_scope\n",
    "\n",
    "# Function to get outputs from each layer.\n",
    "def get_all_outputs(model, input_data, learning_phase=False):\n",
    "    outputs = [layer.output for layer in model.layers] # exclude Input\n",
    "    layers_fn = K.function([model.input, K.symbolic_learning_phase()], outputs)\n",
    "    return layers_fn([input_data, learning_phase])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List for layer names.\n",
    "\n",
    "Layer_Names = []\n",
    "for layer in Lenet.layers:\n",
    "    Layer_Names.append(layer.name)\n",
    "\n",
    "QLayer_Names = []\n",
    "for layer in QLenet.layers:\n",
    "    QLayer_Names.append(layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary with layer name -> outputs\n",
    "Layers_Outputs  = dict(zip(Layer_Names, get_all_outputs(Lenet,image[0])))\n",
    "QLayers_Outputs = dict(zip(QLayer_Names, get_all_outputs(QLenet,image[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Outputs for QDense2 Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.7985363e+00,  2.9917758e+00, -1.9658886e+00, -1.3550131e+00,\n",
       "         4.0264950e+00, -7.2691590e-01,  9.0934563e-01,  5.2751284e+00,\n",
       "        -4.5163706e-01,  1.0568833e+00,  1.3927757e+00, -9.1078562e-01,\n",
       "        -2.3354990e+00, -2.9907271e-01, -1.6706569e+00, -1.5447171e+00,\n",
       "        -1.8712231e+00,  9.3230104e-01, -2.0313833e+00,  1.7453640e+00,\n",
       "         3.2470200e+00,  3.6184146e+00,  2.6308894e+00,  1.2353708e+00,\n",
       "         4.0721264e+00,  2.7568493e+00,  2.8428876e+00, -1.5958537e+00,\n",
       "         3.3470535e+00, -8.5182175e-02, -2.0848680e+00,  2.1405609e+00,\n",
       "        -5.0434155e+00, -1.4105461e+00, -2.3029561e+00, -4.7014804e+00,\n",
       "         1.1015703e-01, -6.4549203e+00,  1.7049364e+00,  2.5652118e+00,\n",
       "         5.0675601e-01,  2.1410992e+00,  2.1490867e+00, -2.6264310e+00,\n",
       "        -3.0487020e+00,  1.6208662e+00,  3.3934338e+00,  4.6600504e+00,\n",
       "         5.4187584e+00, -2.3735075e+00, -6.7448535e+00, -2.7114751e+00,\n",
       "        -1.4448148e+00, -2.9179442e+00, -7.1346298e-02, -1.3467031e+00,\n",
       "         5.2923417e+00,  9.1882426e-01,  9.3517971e-01,  6.0942644e-01,\n",
       "        -7.2785258e-01, -7.0958084e-01,  5.5914154e+00,  1.8275911e+00,\n",
       "        -2.8164587e+00,  5.7137580e+00,  6.4520665e-02,  1.8700029e+00,\n",
       "        -1.8493289e+00,  5.6209650e+00, -2.9174016e+00, -2.9120153e-01,\n",
       "        -4.2984514e+00,  1.0535921e+00, -1.7956644e+00,  1.2000831e+00,\n",
       "        -4.0465164e+00, -2.5387414e+00, -7.8725858e+00, -1.8102210e+00,\n",
       "         1.9675469e+00, -1.4111414e+00,  4.1146353e-03,  2.3587482e+00]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remember that in the Not Quantized model QLayers are just a bypass hence QDense = Dense\n",
    "Layers_Outputs['QDense2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.625,  2.125, -0.75 , -1.625,  4.125, -0.375,  1.25 ,  4.   ,\n",
       "         0.375,  1.25 ,  1.5  , -0.75 , -2.75 ,  0.5  , -1.   , -1.5  ,\n",
       "        -0.875,  1.75 , -2.625,  0.75 ,  3.5  ,  3.   ,  2.5  ,  1.125,\n",
       "         3.875,  3.375,  2.625, -1.5  ,  2.375,  0.875, -2.875,  1.25 ,\n",
       "        -4.875, -1.25 , -1.75 , -5.   ,  0.125, -5.5  ,  1.25 ,  2.625,\n",
       "         0.   ,  1.875,  3.   , -2.375, -2.875,  0.75 ,  3.   ,  4.625,\n",
       "         5.5  , -3.   , -6.   , -2.5  , -1.5  , -2.125, -0.75 , -1.5  ,\n",
       "         4.   , -0.25 ,  0.5  , -0.5  ,  0.   , -1.25 ,  5.75 ,  2.375,\n",
       "        -2.625,  5.375,  0.875,  0.375, -1.625,  5.5  , -2.5  ,  0.125,\n",
       "        -4.625,  0.5  , -2.625,  0.5  , -4.875, -2.25 , -7.375, -1.75 ,\n",
       "         2.25 , -1.75 ,  0.375,  1.875]], dtype=float32)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QLayers_Outputs['QDense2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Max and Min Values of Each Layer for the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_layers = 25\n",
    "iterator  = iter(test_dataset)\n",
    "image     = next(iterator,'Stop')\n",
    "Max_values = [0]*25\n",
    "Min_values = [0]*25\n",
    "while image != 'Stop':\n",
    "    Model_outputs = get_all_outputs(Lenet,image[0])\n",
    "    Max_iteration_values = np.array([np.max(itm) for itm in Model_outputs])\n",
    "    Min_iteration_values = np.array([np.min(itm) for itm in Model_outputs])\n",
    "    Max_values = np.maximum(Max_values, Max_iteration_values)\n",
    "    Min_values = np.minimum(Min_values, Min_iteration_values)\n",
    "    image = next(iterator,'Stop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.        ,  1.        ,  6.51962948,  6.51962948,  0.99999565,\n",
       "        0.99999565,  0.99997681, 11.36214638, 11.36214638,  1.        ,\n",
       "        1.        ,  0.99999976,  0.99999976, 15.4092989 , 15.4092989 ,\n",
       "        1.        ,  1.        , 10.59511375, 10.59511375,  1.        ,\n",
       "        1.        , 15.80233192, 15.80233192,  0.99999881,  0.99999881])"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Max_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.        ,   0.        ,  -3.21593666,  -3.21593666,\n",
       "        -0.9967863 ,  -0.9967863 ,  -0.99342388, -12.16223907,\n",
       "       -12.16223907,  -1.        ,  -1.        ,  -0.99999869,\n",
       "        -0.99999869, -15.66343212, -15.66343212,  -1.        ,\n",
       "        -1.        , -11.51284122, -11.51284122,  -1.        ,\n",
       "        -1.        , -11.09358406, -11.09358406,   0.        ,\n",
       "         0.        ])"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Min_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Max and Min values of Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6256596\n",
      "0.21230586\n",
      "0.9251389\n",
      "0.18390743\n",
      "0.62760293\n",
      "0.24772961\n",
      "0.54453754\n",
      "0.20400645\n",
      "0.72902125\n",
      "0.05587189\n"
     ]
    }
   ],
   "source": [
    "for itm in Lenet.get_weights():\n",
    "    print(np.max(itm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.58532584\n",
      "-0.2912165\n",
      "-0.80219436\n",
      "-0.19225334\n",
      "-0.9469629\n",
      "-0.23552617\n",
      "-0.6235763\n",
      "-0.22716506\n",
      "-0.6461236\n",
      "-0.07768464\n"
     ]
    }
   ],
   "source": [
    "for itm in Lenet.get_weights():\n",
    "    print(np.min(itm))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "OverfittingAndCallbacks.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
