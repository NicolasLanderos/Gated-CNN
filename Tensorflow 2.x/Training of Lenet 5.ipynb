{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "fZOMjk-2pGPs"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "UHY5527vqCZw"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "feuqKVkHqQVW"
   },
   "outputs": [],
   "source": [
    "# Loading Data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "# Adding Channel Lenght: Expanding from (28x28) to (28x28x1)\n",
    "x_train = tf.expand_dims(x_train, -1)\n",
    "x_test = tf.expand_dims(x_test, -1)\n",
    "# Creating validation Subset\n",
    "x_valid = x_train[50000:]  \n",
    "y_valid = y_train[50000:]  \n",
    "\n",
    "x_train = x_train[:50000]\n",
    "y_train = y_train[:50000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dataset Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Z9YPFldhqV5K"
   },
   "outputs": [],
   "source": [
    "#Training\n",
    "# ------------------------------\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "# Shuffle\n",
    "train_dataset = train_dataset.shuffle(buffer_size=x_train.shape[0])\n",
    "\n",
    "# Normalize images\n",
    "def normalize_img(x_, y_):\n",
    "    return tf.cast(x_, tf.float32) / 255., y_\n",
    "\n",
    "# 1-hot encoding <- for categorical cross entropy\n",
    "def to_categorical(x_, y_):\n",
    "    return x_, tf.one_hot(y_, depth=10)\n",
    "\n",
    "train_dataset = train_dataset.map(to_categorical)\n",
    "train_dataset = train_dataset.map(normalize_img)\n",
    "\n",
    "# Divide in batches\n",
    "bs = 32\n",
    "train_dataset = train_dataset.batch(bs)\n",
    "# Repeat\n",
    "# Without calling the repeat function the dataset \n",
    "# will be empty after consuming all the images\n",
    "train_dataset = train_dataset.repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4RT0Za7zqZZE"
   },
   "outputs": [],
   "source": [
    "#Validation   \n",
    "# -----------------------\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((x_valid, y_valid))\n",
    "# Normalize images\n",
    "valid_dataset = valid_dataset.map(normalize_img)\n",
    "# Enconding\n",
    "valid_dataset = valid_dataset.map(to_categorical)\n",
    "# Divide in batches\n",
    "valid_dataset = valid_dataset.batch(bs)\n",
    "# Repeat\n",
    "valid_dataset = valid_dataset.repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ehSGT74aqcAJ"
   },
   "outputs": [],
   "source": [
    "#Testing \n",
    "# -------------------\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_dataset = test_dataset.map(normalize_img)\n",
    "test_dataset = test_dataset.map(to_categorical)\n",
    "test_dataset = test_dataset.batch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "gL2dc0-iqfc4"
   },
   "outputs": [],
   "source": [
    "# Implementation of the original Lenet5 \n",
    "lenet = tf.keras.Sequential()\n",
    "lenet.add(tf.keras.layers.Conv2D(6, kernel_size=5, strides=1,  activation='tanh', input_shape=(28,28,1), padding='same'))\n",
    "lenet.add(tf.keras.layers.AveragePooling2D())\n",
    "lenet.add(tf.keras.layers.Conv2D(16, kernel_size=5, strides=1, activation='tanh', padding='valid'))\n",
    "lenet.add(tf.keras.layers.AveragePooling2D())\n",
    "lenet.add(tf.keras.layers.Flatten())\n",
    "lenet.add(tf.keras.layers.Dense(units=120, activation='tanh'))\n",
    "lenet.add(tf.keras.layers.Dense(units=84, activation='tanh'))\n",
    "lenet.add(tf.keras.layers.Dense(units=10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Pretrained Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "-Q8qN9xeqzm7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 6)         156       \n",
      "_________________________________________________________________\n",
      "average_pooling2d (AveragePo (None, 14, 14, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 10, 10, 16)        2416      \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 5, 5, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               48120     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 61,706\n",
      "Trainable params: 61,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Loading Wieghts\n",
    "#lenet.load_weights(checkpoint_path)\n",
    "\n",
    "# Visualize Lenet 5 Architecture\n",
    "lenet.summary()\n",
    "\n",
    "# Visualize initialized weights\n",
    "#lenet.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "BJjvzefkq3v9"
   },
   "outputs": [],
   "source": [
    "# Optimization params\n",
    "# -------------------\n",
    "\n",
    "# Loss\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "# learning rate\n",
    "lr = 1e-3\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "# -------------------\n",
    "\n",
    "# Validation metrics\n",
    "# ------------------\n",
    "\n",
    "metrics = ['accuracy']\n",
    "# ------------------\n",
    "\n",
    "# Compile Model\n",
    "lenet.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "M7dnyjkoq6xz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Save Directory (Change for your own directory)\n",
    "cwd = 'C:/Users/nicol/Desktop/Keras Testing' \n",
    "\n",
    "# Creating SubDirectory\n",
    "exps_dir = os.path.join(cwd, 'Lenet5')\n",
    "if not os.path.exists(exps_dir):\n",
    "    os.makedirs(exps_dir)\n",
    "\n",
    "now = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "\n",
    "exp_name = \"Lenet5\"\n",
    "\n",
    "exp_dir = os.path.join(exps_dir, exp_name + '_' + str(now))\n",
    "if not os.path.exists(exp_dir):\n",
    "    os.makedirs(exp_dir)\n",
    "    \n",
    "callbacks = []\n",
    "\n",
    "# Model checkpoint\n",
    "# ----------------\n",
    "ckpt_dir = os.path.join(exp_dir, 'ckpts')\n",
    "if not os.path.exists(ckpt_dir):\n",
    "    os.makedirs(ckpt_dir)\n",
    "\n",
    "ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp_{epoch:02d}.ckpt'), \n",
    "                                                   save_weights_only=True)  # False to save the model directly\n",
    "callbacks.append(ckpt_callback)\n",
    "\n",
    "# ----------------\n",
    "\n",
    "# Visualize Learning on Tensorboard\n",
    "# ---------------------------------\n",
    "tb_dir = os.path.join(exp_dir, 'tb_logs')\n",
    "if not os.path.exists(tb_dir):\n",
    "    os.makedirs(tb_dir)\n",
    "    \n",
    "# By default shows losses and metrics for both training and validation\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir,\n",
    "                                             profile_batch=0,\n",
    "                                             histogram_freq=1,write_graph=True,write_images=True,embeddings_freq=1)  \n",
    "callbacks.append(tb_callback)\n",
    "\n",
    "# Early Stopping\n",
    "# --------------\n",
    "early_stop = True\n",
    "if early_stop:\n",
    "    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    callbacks.append(es_callback)\n",
    "\n",
    "# ---------------------------------\n",
    "\n",
    "# How to visualize Tensorboard\n",
    "\n",
    "# 1. tensorboard --logdir EXPERIMENTS_DIR c--port PORT     <- from terminal\n",
    "# 2. localhost:PORT   <- in your browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "nEctDEqduzsE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 1563 steps, validate for 313 steps\n",
      "Epoch 1/100\n",
      "1563/1563 [==============================] - 13s 8ms/step - loss: 0.5310 - accuracy: 0.8049 - val_loss: 0.4116 - val_accuracy: 0.8516\n",
      "Epoch 2/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.3818 - accuracy: 0.8589 - val_loss: 0.3828 - val_accuracy: 0.8556\n",
      "Epoch 3/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.3402 - accuracy: 0.8739 - val_loss: 0.3370 - val_accuracy: 0.8778\n",
      "Epoch 4/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.3112 - accuracy: 0.8832 - val_loss: 0.3254 - val_accuracy: 0.8765\n",
      "Epoch 5/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.2916 - accuracy: 0.8918 - val_loss: 0.3080 - val_accuracy: 0.8864\n",
      "Epoch 6/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.2706 - accuracy: 0.8993 - val_loss: 0.3192 - val_accuracy: 0.8825\n",
      "Epoch 7/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.2564 - accuracy: 0.9050 - val_loss: 0.3007 - val_accuracy: 0.8917\n",
      "Epoch 8/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.2447 - accuracy: 0.9087 - val_loss: 0.3042 - val_accuracy: 0.8914\n",
      "Epoch 9/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.2309 - accuracy: 0.9133 - val_loss: 0.3118 - val_accuracy: 0.8892\n",
      "Epoch 10/100\n",
      "1563/1563 [==============================] - 12s 7ms/step - loss: 0.2193 - accuracy: 0.9182 - val_loss: 0.3036 - val_accuracy: 0.8893\n",
      "Epoch 11/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.2101 - accuracy: 0.9212 - val_loss: 0.3001 - val_accuracy: 0.8912\n",
      "Epoch 12/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.1985 - accuracy: 0.9253 - val_loss: 0.3097 - val_accuracy: 0.8895\n",
      "Epoch 13/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.1897 - accuracy: 0.9294 - val_loss: 0.3033 - val_accuracy: 0.8909\n",
      "Epoch 14/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.1766 - accuracy: 0.9348 - val_loss: 0.3065 - val_accuracy: 0.8929\n",
      "Epoch 15/100\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.1709 - accuracy: 0.9368 - val_loss: 0.3123 - val_accuracy: 0.8904\n",
      "Epoch 16/100\n",
      "1563/1563 [==============================] - 12s 7ms/step - loss: 0.1632 - accuracy: 0.9382 - val_loss: 0.3091 - val_accuracy: 0.8977\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20bb232dc48>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenet.fit(x=train_dataset,\n",
    "          epochs=100,  #### set repeat in training dataset\n",
    "          steps_per_epoch=int(np.ceil(x_train.shape[0] / bs)),\n",
    "          validation_data=valid_dataset,\n",
    "          validation_steps=int(np.ceil(x_valid.shape[0] / bs)), \n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet.save_weights(\"C:/Users/nicol/Desktop/weights/Weights\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterator over test Dataset\n",
    "iterator  = iter(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting new image from iterator\n",
    "image     = next(iterator)\n",
    "image_plt = image[0][0,...,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x20bc3132e88>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting Test image\n",
    "plt.imshow(image_plt, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 2\n",
      "Prediction: 2\n"
     ]
    }
   ],
   "source": [
    "# Target\n",
    "tf.print(\"Target:\",np.argmax(image[1]))\n",
    "# Predicted Output\n",
    "print(\"Prediction:\",np.argmax(lenet.predict(image[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classes\n",
    "\n",
    "0. T-shirt/top\n",
    "1. Trouser\n",
    "2. Pullover\n",
    "3. Dress\n",
    "4. Coat\n",
    "5. Sandal\n",
    "6. Shirt\n",
    "7. Sneaker\n",
    "8. Bag\n",
    "9. Ankle boot"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "OverfittingAndCallbacks.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
